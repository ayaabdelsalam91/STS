Model: Our system employs a ridge regression model (linear regression with L2 error and L2 regularization) to combine a set of similarity measures.
Feature: Word embbedings averaging as a baseline
		Word Alignment (the similarity in the paper)
		skip-thoughts vetors cosine similarity

The model is trained on SemEval 2012–2015 data.

Baseline :
Paragram embeddings were used ,  first we search for the word in Paragram-Phrase XXL if word was not found we searched in  Paragram-SL999
We averaged the words vectors to get senctence2vector representation
We used cosine similarity to get similarty between words.
Preprocessing: 
-lower case
-remove special chars
-replace \ with spaces
-remove number


Anaylsis:
#to be done by Dantong and Jiani
Results:

answer-answer   0.650946172146
question-question   0.670415129392
postediting   0.841269227473
plagiarism   0.823757533664
headlines   0.681947849525
0.73366718244


Fancy:
Data preprcessing : for feature 1 and 3 -lower case
										-remove special chars
										-replace \ with spaces
										-remove url
										-remove emails
										-Use regular expressions in Python to find and replace all text contractions with their respective complete text. For example, “wouldn’t” is replaced with “would not”
										-remove parenthetical expressions
					for feature 2 		all mentioned above plus: 
										-remove stop words
										-remove number

Feature 1 was word embeddings:
Same used in base line 

Feature 2 Word Alignment:
We used apply the monolingual word aligner developed by Sultan et al. (2014a) to input sentence pairs. Similarity between senetce was calcuted by (copy the equation in the paper)

Feature 3 Skip-thoughts:
We used the concatenation of uni-skip and bi-skip, resulting in a 4800 dimensional vector represenation for each sentence. (we used pretrained )
Cosine similarty was used to calucalte the similarty between this pair of vectors

Results:

feature 1 alone:
answer-answer   0.69505021323
question-question   0.664533239058
postediting   0.840178840444
plagiarism   0.831547137089
headlines   0.741949762189
Average 0.754651838402

Comibing feature 1 and 2:

answer-answer   0.670337260766
question-question   0.637645411677
postediting   0.836145195043
plagiarism   0.831443012915
headlines   0.768850807066
Average  0.748884337493

Combinig feature 1 and 2 and 3:

answer-answer   0.645911980027
question-question   0.625908092321
postediting   0.836426014612
plagiarism   0.835382164036
headlines   0.76855060249
0.742435770697

Combinig feature 1 and 3:

answer-answer   0.695059428014
question-question   0.664413633177
postediting   0.840244803483
plagiarism   0.831558165818
headlines   0.741949588484
0.754645123795

Combinig feature 2 and 3:

answer-answer   0.540158623699
question-question   0.521710131668
postediting   0.817066091062
plagiarism   0.810176308663
headlines   0.753785334636
0.688579297946


Best output was feature 1 only

An error analysis of your best performing results
//to be done by someone
What else you could have tried if you had more time 
//to be done by someone


Dont forget to add refences skipgram paper the word embbedding paper and the paper jani sent me


